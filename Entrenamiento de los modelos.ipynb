{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e74a609f",
   "metadata": {},
   "source": [
    "# primer paso\n",
    "lo que hacemos aqui es ver si el problema al que nos enfrentamos es multiclase o es multietiqueta, eso nos sirve para ver el mejor aproach al momento de entrenar el modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93c06914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path          # Manejo de rutas (más limpio que strings)\n",
    "import pandas as pd               # Leer el CSV y manipular data tabular\n",
    "import numpy as np                # Operaciones numéricas, arreglos\n",
    "from PIL import Image, ImageFile  # Cargar imágenes .jpg/.png de disco\n",
    "\n",
    "import torch                      # Núcleo de PyTorch (tensores, device)\n",
    "from torch import nn              # Capas y pérdidas (Linear, CrossEntropyLoss, etc.)\n",
    "from torch.utils.data import Dataset, DataLoader  # Dataset/DataLoader\n",
    "import torchvision                # Ecosistema visión en PyTorch\n",
    "from torchvision import transforms as T, models   # Preprocesos y modelos preentrenados\n",
    "\n",
    "from collections import defaultdict, Counter      # Diccionarios útiles (bolsas por lesión, conteos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab53f08",
   "metadata": {},
   "source": [
    "Es un problema multiclase, con cada foto representando solo una enfermedad y no muchas compartidas, con esto ya podemos comenzar a realizar el dataset junto con las etiquetas para lograr entrenar el modelo\n",
    "\n",
    "# creacion del data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f0f52b",
   "metadata": {},
   "source": [
    "Este data set, espero haber descargado el correcto desde la pagina, tiene 2 imagenes por lesion, una de lejos y otra de cerca, primero pense en usar solo 1 imagen por separado, y ya ver de esas cuantas el modelo predecia bien, pero creo que en un consultorio un medico podria tomar varias fotos de la malformacion, por ello creo que es mejor entrenarlas en conjunto, es decir buscar la forma de usar las 2 imagenes para una sola prediccion. por ello en este paso crearemos primero un data set que nos permita hacer eso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50377be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permitir abrir JPGs incompletos (común en copias a USB)\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Rutas (ajusta a las tuyas)\n",
    "DATA_DIR = Path(\"/Users/kevinjiro/Desktop/data Piel/Train/MILK10k_Training_Input\") # carpeta con imágenes\n",
    "CSV_PATH  = Path(\"/Users/kevinjiro/Desktop/data Piel/Train/MILK10k_Training_GroundTruth.csv\") # CSV con etiquetas\n",
    "TEST_DIR  = Path(\"/Users/kevinjiro/Desktop/data Piel/test\")   # lo reservamos para más adelante\n",
    "\n",
    "# CPU vs GPU (usa GPU si está disponible)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reproducibilidad razonable (semillas)\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0da0c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 11  # placeholder; lo calcularemos del CSV en el siguiente paso\n",
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)  # pesos ImageNet\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)  # re-cabeceamos a tus clases\n",
    "model = model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ee8968f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases: ['AKIEC', 'BCC', 'BEN_OTH', 'BKL', 'DF', 'INF', 'MAL_OTH', 'MEL', 'NV', 'SCCKA', 'VASC']\n",
      "Número de clases: 11\n",
      "Ejemplo: [('IL_0000652', 1), ('IL_0003176', 1), ('IL_0004688', 1), ('IL_0005081', 9), ('IL_0006177', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Leer el CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Columnas de clases (todas menos lesion_id)\n",
    "class_cols = [c for c in df.columns if c != \"lesion_id\"]\n",
    "num_classes = len(class_cols)\n",
    "print(\"Clases:\", class_cols)\n",
    "print(\"Número de clases:\", num_classes)\n",
    "\n",
    "# Vector numpy con las clases (one-hot)\n",
    "labels_array = df[class_cols].values\n",
    "\n",
    "# Diccionario lesion_id -> índice de clase\n",
    "lesion2label = {\n",
    "    lid: int(labels_array[i].argmax())\n",
    "    for i, lid in enumerate(df[\"lesion_id\"])\n",
    "}\n",
    "\n",
    "# Chequeo rápido\n",
    "print(\"Ejemplo:\", list(lesion2label.items())[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f57294dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total imágenes encontradas: 10480\n",
      "Ejemplo de sample: (PosixPath('/Users/kevinjiro/Desktop/data Piel/Train/MILK10k_Training_Input/IL_0000652/ISIC_4671410.jpg'), 1, 'IL_0000652')\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "\n",
    "for lid, label_idx in lesion2label.items():\n",
    "    folder = DATA_DIR / lid\n",
    "    if not folder.exists():\n",
    "        continue\n",
    "    for img_path in folder.glob(\"*.jpg\"):\n",
    "        if img_path.name.startswith(\"._\"):   # ignorar basura de macOS\n",
    "            continue\n",
    "        samples.append((img_path, label_idx, lid))\n",
    "\n",
    "print(\"Total imágenes encontradas:\", len(samples))\n",
    "print(\"Ejemplo de sample:\", samples[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "374c97d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lesiones con >1 imagen: 5240\n",
      "Ejemplo (una lesión con varias imágenes): ('IL_0000652', 2)\n",
      "Lesiones train: 4197 | imágenes train: 8394\n",
      "Lesiones val: 1043 | imágenes val: 2086\n",
      "Intersección de lesion_id: set()\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "# ¿cuántas imágenes por lesión quedaron en samples?\n",
    "imgs_por_lesion = Counter([lid for _, _, lid in samples])\n",
    "print(\"Lesiones con >1 imagen:\", sum(v>1 for v in imgs_por_lesion.values()))\n",
    "print(\"Ejemplo (una lesión con varias imágenes):\",\n",
    "      next(((lid, imgs_por_lesion[lid]) for lid in imgs_por_lesion if imgs_por_lesion[lid]>1), \"Ninguna\"))\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "val_frac = 0.2\n",
    "\n",
    "# 1) agrupar lesiones por clase\n",
    "class2lesions = defaultdict(list)\n",
    "for lid, y in lesion2label.items():\n",
    "    # solo lesiones que sí tienen imágenes en samples\n",
    "    # (más robusto: derivarlo de imgs_por_lesion)\n",
    "    if imgs_por_lesion[lid] > 0:\n",
    "        class2lesions[y].append(lid)\n",
    "\n",
    "# 2) split estratificado por clase (por lesion_id)\n",
    "train_lids, val_lids = [], []\n",
    "for y, lids in class2lesions.items():\n",
    "    lids = np.array(lids)\n",
    "    rng.shuffle(lids)\n",
    "    n_val = max(1, int(len(lids)*val_frac))\n",
    "    val_lids.extend(lids[:n_val].tolist())\n",
    "    train_lids.extend(lids[n_val:].tolist())\n",
    "\n",
    "# 3) expandir a nivel imagen\n",
    "train_samples, val_samples = [], []\n",
    "for p, y, lid in samples:\n",
    "    if lid in set(train_lids):\n",
    "        train_samples.append((p, y, lid))\n",
    "    elif lid in set(val_lids):\n",
    "        val_samples.append((p, y, lid))\n",
    "\n",
    "print(\"Lesiones train:\", len(set(train_lids)), \"| imágenes train:\", len(train_samples))\n",
    "print(\"Lesiones val:\",   len(set(val_lids)),   \"| imágenes val:\",   len(val_samples))\n",
    "print(\"Intersección de lesion_id:\", set(train_lids).intersection(val_lids))  # debe ser set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f6e2057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "img_size = 224\n",
    "\n",
    "train_tfms = T.Compose([\n",
    "    T.Resize((img_size, img_size)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomRotation(degrees=10),\n",
    "    T.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.1, hue=0.02),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "val_tfms = T.Compose([\n",
    "    T.Resize((img_size, img_size)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "158770d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb: torch.Size([64, 3, 224, 224])\n",
      "yb: torch.Size([64]) tensor([1, 1, 8, 8, 1])\n",
      "lid ejemplo: ('IL_0003176', 'IL_0003176', 'IL_0008891', 'IL_0008891', 'IL_0019048')\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  # abre JPGs truncados sin romper\n",
    "\n",
    "class LesionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    samples: lista de tuplas (ruta_imagen, class_idx, lesion_id)\n",
    "    \"\"\"\n",
    "    def __init__(self, samples, transforms):\n",
    "        self.samples = samples\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, y, lid = self.samples[idx]\n",
    "        try:\n",
    "            x = Image.open(path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error abriendo {path}: {e}\")\n",
    "        x = self.transforms(x)\n",
    "        y = torch.tensor(y, dtype=torch.long)  # CrossEntropy → entero\n",
    "        return x, y, str(lid)\n",
    "\n",
    "# Instancias\n",
    "train_ds = LesionDataset(train_samples, train_tfms)\n",
    "val_ds   = LesionDataset(val_samples,   val_tfms)\n",
    "\n",
    "# Para depurar primero con 0 workers\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=0, pin_memory=False)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=64, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "# Quick check\n",
    "xb, yb, lidb = next(iter(val_dl))\n",
    "print(\"xb:\", xb.shape)           # (B, 3, 224, 224)\n",
    "print(\"yb:\", yb.shape, yb[:5])   # (B,)\n",
    "print(\"lid ejemplo:\", lidb[:5])  # strings de lesion_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e463aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "num_classes = len(class_cols)  # de tu CSV leído antes\n",
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "667dc5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'img_acc': 0.1860019175455417, 'img_loss': 2.300363646257644, 'lesion_acc': 0.20038350910834132, 'n_images': 2086, 'n_lesions': 1043}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_late_fusion(model, loader, device=DEVICE):\n",
    "    model.eval()\n",
    "    total_img, correct_img, loss_img = 0, 0, 0.0\n",
    "\n",
    "    lesion_logits_sum = defaultdict(lambda: torch.zeros(num_classes, device=device))\n",
    "    lesion_counts     = defaultdict(int)\n",
    "    lesion_targets    = {}\n",
    "\n",
    "    for xb, yb, lidb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(xb)                    # (B, C)\n",
    "        loss   = criterion(logits, yb)\n",
    "        loss_img += loss.item() * xb.size(0)\n",
    "\n",
    "        preds_img = logits.argmax(dim=1)\n",
    "        correct_img += (preds_img == yb).sum().item()\n",
    "        total_img   += xb.size(0)\n",
    "\n",
    "        for j, lid in enumerate(lidb):\n",
    "            lesion_logits_sum[lid] += logits[j]\n",
    "            lesion_counts[lid]     += 1\n",
    "            lesion_targets[lid]     = int(yb[j].item())\n",
    "\n",
    "    # métricas por lesión\n",
    "    correct_lesion, total_lesion = 0, 0\n",
    "    for lid, zsum in lesion_logits_sum.items():\n",
    "        zmean = zsum / lesion_counts[lid]\n",
    "        pred  = int(zmean.argmax().item())\n",
    "        ytrue = lesion_targets[lid]\n",
    "        correct_lesion += int(pred == ytrue)\n",
    "        total_lesion   += 1\n",
    "\n",
    "    img_acc    = correct_img / max(1, total_img)\n",
    "    img_loss   = loss_img / max(1, total_img)\n",
    "    lesion_acc = correct_lesion / max(1, total_lesion)\n",
    "\n",
    "    return {\"img_acc\": img_acc, \"img_loss\": img_loss, \"lesion_acc\": lesion_acc,\n",
    "            \"n_images\": total_img, \"n_lesions\": total_lesion}\n",
    "\n",
    "# Prueba (sin entrenar aún; la acc será baja, sólo validamos el pipeline)\n",
    "metrics = evaluate_late_fusion(model, val_dl)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "370b1c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_late_fusion_with_preds(\n",
    "    model,\n",
    "    loader,\n",
    "    num_classes: int,\n",
    "    criterion,\n",
    "    device=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Devuelve:\n",
    "      - metrics: dict con acc/loss a nivel imagen y lesión\n",
    "      - y_true_lesion: lista[int] con la clase real por lesión\n",
    "      - y_pred_lesion: lista[int] con la clase predicha por lesión (late fusion)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    total_img, correct_img, loss_img = 0, 0, 0.0\n",
    "\n",
    "    # Acumuladores por lesión\n",
    "    lesion_logits_sum = defaultdict(lambda: torch.zeros(num_classes, device=device))\n",
    "    lesion_counts     = defaultdict(int)\n",
    "    lesion_targets    = {}\n",
    "\n",
    "    for xb, yb, lidb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(xb)                 # (B, C)\n",
    "        loss   = criterion(logits, yb)\n",
    "        loss_img += loss.item() * xb.size(0)\n",
    "\n",
    "        preds_img = logits.argmax(1)\n",
    "        correct_img += (preds_img == yb).sum().item()\n",
    "        total_img   += xb.size(0)\n",
    "\n",
    "        # acumular por lesión (late fusion)\n",
    "        for j, lid in enumerate(lidb):\n",
    "            lesion_logits_sum[lid] += logits[j]\n",
    "            lesion_counts[lid]     += 1\n",
    "            lesion_targets[lid]     = int(yb[j].item())\n",
    "\n",
    "    # construye y_true / y_pred por lesión\n",
    "    y_true_lesion: List[int] = []\n",
    "    y_pred_lesion: List[int] = []\n",
    "    for lid, zsum in lesion_logits_sum.items():\n",
    "        zmean = zsum / lesion_counts[lid]\n",
    "        y_pred = int(zmean.argmax().item())\n",
    "        y_true = lesion_targets[lid]\n",
    "        y_pred_lesion.append(y_pred)\n",
    "        y_true_lesion.append(y_true)\n",
    "\n",
    "    metrics = {\n",
    "        \"img_acc\": correct_img / max(1, total_img),\n",
    "        \"img_loss\": loss_img / max(1, total_img),\n",
    "        \"lesion_acc\": (sum(int(p==t) for p,t in zip(y_pred_lesion,y_true_lesion))\n",
    "                       / max(1, len(y_true_lesion))),\n",
    "        \"n_images\": total_img,\n",
    "        \"n_lesions\": len(y_true_lesion),\n",
    "    }\n",
    "    return metrics, y_true_lesion, y_pred_lesion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d512c47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'img_acc': 0.1860019175455417, 'img_loss': 2.300363646257644, 'lesion_acc': 0.20038350910834132, 'n_images': 2086, 'n_lesions': 1043}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       AKIEC      0.083     0.017     0.028        60\n",
      "         BCC      0.453     0.258     0.329       504\n",
      "     BEN_OTH      0.000     0.000     0.000         8\n",
      "         BKL      0.140     0.259     0.182       108\n",
      "          DF      0.000     0.000     0.000        10\n",
      "         INF      0.000     0.000     0.000        10\n",
      "     MAL_OTH      0.000     0.000     0.000         1\n",
      "         MEL      0.078     0.333     0.127        90\n",
      "          NV      0.000     0.000     0.000       149\n",
      "       SCCKA      0.182     0.213     0.196        94\n",
      "        VASC      0.000     0.000     0.000         9\n",
      "\n",
      "    accuracy                          0.200      1043\n",
      "   macro avg      0.085     0.098     0.078      1043\n",
      "weighted avg      0.261     0.200     0.208      1043\n",
      "\n",
      "[[  1   7   1  18   0   0   1  25   0   7   0]\n",
      " [  3 130   0 106   0   2   0 201   0  44  18]\n",
      " [  0   2   0   2   0   0   0   4   0   0   0]\n",
      " [  3  34   0  28   0   0   0  26   0  13   4]\n",
      " [  0   2   0   2   0   0   0   3   0   1   2]\n",
      " [  0   1   0   2   0   0   1   5   0   0   1]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0]\n",
      " [  3  31   0  10   0   1   0  30   0  12   3]\n",
      " [  0  71   0  15   0   7   0  38   0  12   6]\n",
      " [  2   8   0  16   0   0   1  46   0  20   1]\n",
      " [  0   1   0   1   0   0   0   6   1   0   0]]\n",
      "F1-macro (lesión): 0.07826859275193326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinjiro/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kevinjiro/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kevinjiro/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics, y_true, y_pred = evaluate_late_fusion_with_preds(\n",
    "    model, val_dl, num_classes=len(class_cols), criterion=criterion, device=DEVICE\n",
    ")\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "print(metrics)\n",
    "print(classification_report(y_true, y_pred, target_names=class_cols, digits=3))\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# F1 macro por si lo quieres suelto\n",
    "print(\"F1-macro (lesión):\", f1_score(y_true, y_pred, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3245cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "def build_model(model_name: str, num_classes: int, device=DEVICE):\n",
    "    model_name = model_name.lower()\n",
    "    if model_name == \"resnet18\":\n",
    "        model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        in_feats = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_feats, num_classes)\n",
    "\n",
    "    elif model_name == \"mobilenetv3_small\":\n",
    "        model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "        in_feats = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(in_feats, num_classes)\n",
    "\n",
    "    elif model_name == \"efficientnet_b0\":\n",
    "        model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        in_feats = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(in_feats, num_classes)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Modelo no soportado: {model_name}\")\n",
    "\n",
    "    return model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cacab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/q7r819nd6z34h_qv3b3b0hh00000gn/T/ipykernel_31475/1120316542.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
      "/var/folders/nk/q7r819nd6z34h_qv3b3b0hh00000gn/T/ipykernel_31475/3166561070.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[resnet18] Epoch 01 | train_img_acc=0.576 train_img_loss=1.332 || val_img_acc=0.573 val_lesion_acc=0.593 val_img_loss=1.491 | lr=1.00e-03\n",
      "[resnet18] Epoch 02 | train_img_acc=0.604 train_img_loss=1.214 || val_img_acc=0.593 val_lesion_acc=0.621 val_img_loss=1.299 | lr=1.00e-03\n",
      "[resnet18] Epoch 03 | train_img_acc=0.625 train_img_loss=1.148 || val_img_acc=0.581 val_lesion_acc=0.606 val_img_loss=1.215 | lr=1.00e-03\n",
      "[resnet18] Epoch 04 | train_img_acc=0.636 train_img_loss=1.112 || val_img_acc=0.625 val_lesion_acc=0.654 val_img_loss=1.097 | lr=1.00e-03\n",
      "[resnet18] Epoch 05 | train_img_acc=0.651 train_img_loss=1.065 || val_img_acc=0.636 val_lesion_acc=0.647 val_img_loss=1.140 | lr=1.00e-03\n",
      "[resnet18] Epoch 06 | train_img_acc=0.652 train_img_loss=1.041 || val_img_acc=0.641 val_lesion_acc=0.661 val_img_loss=1.081 | lr=1.00e-03\n",
      "[resnet18] Epoch 07 | train_img_acc=0.656 train_img_loss=1.024 || val_img_acc=0.642 val_lesion_acc=0.667 val_img_loss=1.104 | lr=1.00e-03\n",
      "[resnet18] Epoch 08 | train_img_acc=0.671 train_img_loss=0.979 || val_img_acc=0.636 val_lesion_acc=0.661 val_img_loss=1.133 | lr=1.00e-03\n",
      "[resnet18] Epoch 09 | train_img_acc=0.680 train_img_loss=0.954 || val_img_acc=0.647 val_lesion_acc=0.665 val_img_loss=1.151 | lr=1.00e-03\n",
      "[resnet18] Epoch 10 | train_img_acc=0.693 train_img_loss=0.915 || val_img_acc=0.637 val_lesion_acc=0.675 val_img_loss=1.108 | lr=1.00e-03\n",
      "[resnet18] Mejor checkpoint restaurado (lesion_acc=0.675)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /Users/kevinjiro/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.83M/9.83M [00:01<00:00, 5.33MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mobilenetv3_small] Epoch 01 | train_img_acc=0.609 train_img_loss=1.201 || val_img_acc=0.651 val_lesion_acc=0.664 val_img_loss=1.080 | lr=1.00e-03\n",
      "[mobilenetv3_small] Epoch 02 | train_img_acc=0.656 train_img_loss=1.024 || val_img_acc=0.643 val_lesion_acc=0.685 val_img_loss=1.050 | lr=1.00e-03\n",
      "[mobilenetv3_small] Epoch 03 | train_img_acc=0.682 train_img_loss=0.952 || val_img_acc=0.657 val_lesion_acc=0.689 val_img_loss=1.020 | lr=1.00e-03\n",
      "[mobilenetv3_small] Epoch 04 | train_img_acc=0.695 train_img_loss=0.892 || val_img_acc=0.644 val_lesion_acc=0.691 val_img_loss=1.060 | lr=1.00e-03\n",
      "[mobilenetv3_small] Epoch 05 | train_img_acc=0.715 train_img_loss=0.837 || val_img_acc=0.655 val_lesion_acc=0.690 val_img_loss=1.046 | lr=1.00e-03\n",
      "[mobilenetv3_small] Epoch 06 | train_img_acc=0.734 train_img_loss=0.774 || val_img_acc=0.644 val_lesion_acc=0.683 val_img_loss=1.129 | lr=1.00e-03\n",
      "[mobilenetv3_small] Epoch 07 | train_img_acc=0.746 train_img_loss=0.722 || val_img_acc=0.634 val_lesion_acc=0.680 val_img_loss=1.115 | lr=5.00e-04\n",
      "[mobilenetv3_small] Epoch 08 | train_img_acc=0.808 train_img_loss=0.545 || val_img_acc=0.669 val_lesion_acc=0.721 val_img_loss=1.075 | lr=5.00e-04\n",
      "[mobilenetv3_small] Epoch 09 | train_img_acc=0.836 train_img_loss=0.466 || val_img_acc=0.662 val_lesion_acc=0.711 val_img_loss=1.160 | lr=5.00e-04\n",
      "[mobilenetv3_small] Epoch 10 | train_img_acc=0.852 train_img_loss=0.413 || val_img_acc=0.634 val_lesion_acc=0.699 val_img_loss=1.260 | lr=5.00e-04\n",
      "[mobilenetv3_small] Mejor checkpoint restaurado (lesion_acc=0.721)\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /Users/kevinjiro/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20.5M/20.5M [00:03<00:00, 6.66MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[efficientnet_b0] Epoch 01 | train_img_acc=0.615 train_img_loss=1.221 || val_img_acc=0.655 val_lesion_acc=0.681 val_img_loss=1.074 | lr=1.00e-03\n",
      "[efficientnet_b0] Epoch 02 | train_img_acc=0.664 train_img_loss=1.027 || val_img_acc=0.672 val_lesion_acc=0.701 val_img_loss=0.994 | lr=1.00e-03\n",
      "[efficientnet_b0] Epoch 03 | train_img_acc=0.691 train_img_loss=0.922 || val_img_acc=0.678 val_lesion_acc=0.717 val_img_loss=0.959 | lr=1.00e-03\n",
      "[efficientnet_b0] Epoch 04 | train_img_acc=0.709 train_img_loss=0.865 || val_img_acc=0.689 val_lesion_acc=0.737 val_img_loss=0.949 | lr=1.00e-03\n",
      "[efficientnet_b0] Epoch 05 | train_img_acc=0.726 train_img_loss=0.807 || val_img_acc=0.678 val_lesion_acc=0.728 val_img_loss=0.956 | lr=1.00e-03\n",
      "[efficientnet_b0] Epoch 06 | train_img_acc=0.741 train_img_loss=0.741 || val_img_acc=0.617 val_lesion_acc=0.680 val_img_loss=1.134 | lr=1.00e-03\n",
      "[efficientnet_b0] Epoch 07 | train_img_acc=0.766 train_img_loss=0.678 || val_img_acc=0.682 val_lesion_acc=0.724 val_img_loss=0.986 | lr=5.00e-04\n",
      "[efficientnet_b0] Epoch 08 | train_img_acc=0.825 train_img_loss=0.490 || val_img_acc=0.694 val_lesion_acc=0.756 val_img_loss=1.040 | lr=5.00e-04\n",
      "[efficientnet_b0] Epoch 09 | train_img_acc=0.853 train_img_loss=0.408 || val_img_acc=0.709 val_lesion_acc=0.756 val_img_loss=1.069 | lr=5.00e-04\n",
      "[efficientnet_b0] Epoch 10 | train_img_acc=0.880 train_img_loss=0.344 || val_img_acc=0.691 val_lesion_acc=0.749 val_img_loss=1.168 | lr=5.00e-04\n",
      "[efficientnet_b0] Mejor checkpoint restaurado (lesion_acc=0.756)\n",
      "\n",
      "=== COMPARACIÓN DE MODELOS (ordenado por mejor lesion_acc) ===\n",
      "            model  epoch     lr  train_img_acc  train_img_loss  val_img_acc  val_img_loss  val_lesion_acc  best_lesion_acc\n",
      "  efficientnet_b0     10 0.0005       0.879676        0.344381     0.691275      1.167861        0.748802         0.756472\n",
      "mobilenetv3_small     10 0.0005       0.852395        0.413084     0.634228      1.260482        0.698945         0.720997\n",
      "         resnet18     10 0.0010       0.692876        0.915138     0.637105      1.107790        0.674976         0.674976\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "def train_and_validate_model(model_name, epochs=10, base_lr=1e-3):\n",
    "    # Construir modelo\n",
    "    model = build_model(model_name, num_classes=num_classes, device=DEVICE)\n",
    "\n",
    "    # Optimizador y scheduler (idénticos a tu setup)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=base_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"max\", factor=0.5, patience=2\n",
    "    )\n",
    "    scaler = GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
    "\n",
    "    # Reusar train_one_epoch tal cual (usa scaler global si lo necesitas dentro)\n",
    "    history = []\n",
    "    best_lesion_acc = -1.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_metrics = train_one_epoch(model, train_dl, optimizer, criterion, device=DEVICE)\n",
    "        val_metrics   = evaluate_late_fusion(model, val_dl, device=DEVICE)\n",
    "\n",
    "        # Scheduler según tu métrica clínica\n",
    "        scheduler.step(val_metrics[\"lesion_acc\"])\n",
    "\n",
    "        # Guardar mejor estado\n",
    "        if val_metrics[\"lesion_acc\"] > best_lesion_acc:\n",
    "            best_lesion_acc = val_metrics[\"lesion_acc\"]\n",
    "            best_state = {\n",
    "                \"model\": copy.deepcopy(model.state_dict()),\n",
    "                \"optimizer\": copy.deepcopy(optimizer.state_dict()),\n",
    "                \"epoch\": epoch,\n",
    "                \"val_metrics\": val_metrics,\n",
    "                \"model_name\": model_name\n",
    "            }\n",
    "\n",
    "        # Logging\n",
    "        lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "        row = {\"epoch\": epoch, \"model\": model_name, \"lr\": lr_now, **train_metrics, **val_metrics}\n",
    "        history.append(row)\n",
    "        print(f\"[{model_name}] Epoch {epoch:02d} | \"\n",
    "              f\"train_img_acc={train_metrics['train_img_acc']:.3f} \"\n",
    "              f\"train_img_loss={train_metrics['train_img_loss']:.3f} || \"\n",
    "              f\"val_img_acc={val_metrics['img_acc']:.3f} \"\n",
    "              f\"val_lesion_acc={val_metrics['lesion_acc']:.3f} \"\n",
    "              f\"val_img_loss={val_metrics['img_loss']:.3f} | lr={lr_now:.2e}\")\n",
    "\n",
    "    # Restaurar mejor y guardar a disco\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state[\"model\"])\n",
    "        torch.save(best_state, f\"best_{model_name}.pth\")\n",
    "        print(f\"[{model_name}] Mejor checkpoint restaurado (lesion_acc={best_lesion_acc:.3f})\")\n",
    "\n",
    "    return model, pd.DataFrame(history), best_state\n",
    "\n",
    "# === Ejecutar comparación ===\n",
    "results = []\n",
    "best_states = {}\n",
    "\n",
    "for name in [\"resnet18\", \"mobilenetv3_small\", \"efficientnet_b0\"]:\n",
    "    model_trained, hist_df, best_state = train_and_validate_model(name, epochs=EPOCHS, base_lr=1e-3)\n",
    "    best_states[name] = best_state\n",
    "    # Tomar última fila y además guardar la mejor métrica por-lesión\n",
    "    last = hist_df.iloc[-1].to_dict()\n",
    "    last[\"best_lesion_acc\"] = best_state[\"val_metrics\"][\"lesion_acc\"] if best_state else None\n",
    "    results.append(last)\n",
    "\n",
    "# Tabla comparativa (última época + mejor lesion_acc)\n",
    "compare_df = pd.DataFrame(results)[[\n",
    "    \"model\", \"epoch\", \"lr\", \"train_img_acc\", \"train_img_loss\",\n",
    "    \"img_acc\", \"img_loss\", \"lesion_acc\", \"best_lesion_acc\"\n",
    "]].rename(columns={\n",
    "    \"img_acc\": \"val_img_acc\",\n",
    "    \"img_loss\": \"val_img_loss\",\n",
    "    \"lesion_acc\": \"val_lesion_acc\"\n",
    "}).sort_values(by=\"best_lesion_acc\", ascending=False)\n",
    "\n",
    "print(\"\\n=== COMPARACIÓN DE MODELOS (ordenado por mejor lesion_acc) ===\")\n",
    "print(compare_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34bd4129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESNET18 ===\n",
      "\n",
      "Matriz de confusión (filas=real, columnas=predicha):\n",
      "         AKIEC  BCC  BEN_OTH  BKL  DF  INF  MAL_OTH  MEL   NV  SCCKA  VASC\n",
      "AKIEC        0   27        0   18   0    0        0    0    1     14     0\n",
      "BCC          0  453        0   11   0    0        0    4    2     34     0\n",
      "BEN_OTH      0    4        0    2   0    0        0    2    0      0     0\n",
      "BKL          0   40        0   32   0    0        0    7   17     12     0\n",
      "DF           0    5        0    2   0    0        0    2    1      0     0\n",
      "INF          0    8        0    2   0    0        0    0    0      0     0\n",
      "MAL_OTH      0    0        0    1   0    0        0    0    0      0     0\n",
      "MEL          0    8        0   16   0    0        0   37   28      1     0\n",
      "NV           0   17        0    9   0    0        0   11  111      1     0\n",
      "SCCKA        0   21        0    3   0    0        0    1    0     69     0\n",
      "VASC         0    4        0    1   0    0        0    0    2      0     2\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score      support\n",
      "AKIEC          0.000000  0.000000  0.000000    60.000000\n",
      "BCC            0.771721  0.898810  0.830431   504.000000\n",
      "BEN_OTH        0.000000  0.000000  0.000000     8.000000\n",
      "BKL            0.329897  0.296296  0.312195   108.000000\n",
      "DF             0.000000  0.000000  0.000000    10.000000\n",
      "INF            0.000000  0.000000  0.000000    10.000000\n",
      "MAL_OTH        0.000000  0.000000  0.000000     1.000000\n",
      "MEL            0.578125  0.411111  0.480519    90.000000\n",
      "NV             0.685185  0.744966  0.713826   149.000000\n",
      "SCCKA          0.526718  0.734043  0.613333    94.000000\n",
      "VASC           1.000000  0.222222  0.363636     9.000000\n",
      "accuracy       0.674976  0.674976  0.674976     0.674976\n",
      "macro avg      0.353786  0.300677  0.301267  1043.000000\n",
      "weighted avg   0.610941  0.674976  0.635462  1043.000000\n",
      "\n",
      "Guardado: eval_train_split/confusion_matrix_resnet18.csv\n",
      "Guardado: eval_train_split/classification_report_resnet18.csv\n",
      "\n",
      "=== MOBILENETV3_SMALL ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinjiro/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kevinjiro/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kevinjiro/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusión (filas=real, columnas=predicha):\n",
      "         AKIEC  BCC  BEN_OTH  BKL  DF  INF  MAL_OTH  MEL   NV  SCCKA  VASC\n",
      "AKIEC       20   25        0    8   0    0        0    1    0      6     0\n",
      "BCC         11  465        0    3   0    0        0    3    2     18     2\n",
      "BEN_OTH      0    1        2    1   0    0        0    1    2      0     1\n",
      "BKL          9   37        0   34   1    0        0   13   10      4     0\n",
      "DF           0    5        0    1   3    0        0    1    0      0     0\n",
      "INF          1    4        0    2   0    0        0    1    1      0     1\n",
      "MAL_OTH      0    0        0    0   0    0        0    1    0      0     0\n",
      "MEL          2    6        0    6   0    0        0   54   21      0     1\n",
      "NV           0   14        0    7   1    0        0   12  112      1     2\n",
      "SCCKA        3   24        0   10   0    0        0    0    0     57     0\n",
      "VASC         0    1        0    0   1    0        0    1    1      0     5\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score      support\n",
      "AKIEC          0.434783  0.333333  0.377358    60.000000\n",
      "BCC            0.798969  0.922619  0.856354   504.000000\n",
      "BEN_OTH        1.000000  0.250000  0.400000     8.000000\n",
      "BKL            0.472222  0.314815  0.377778   108.000000\n",
      "DF             0.500000  0.300000  0.375000    10.000000\n",
      "INF            0.000000  0.000000  0.000000    10.000000\n",
      "MAL_OTH        0.000000  0.000000  0.000000     1.000000\n",
      "MEL            0.613636  0.600000  0.606742    90.000000\n",
      "NV             0.751678  0.751678  0.751678   149.000000\n",
      "SCCKA          0.662791  0.606383  0.633333    94.000000\n",
      "VASC           0.416667  0.555556  0.476190     9.000000\n",
      "accuracy       0.720997  0.720997  0.720997     0.720997\n",
      "macro avg      0.513704  0.421308  0.441312  1043.000000\n",
      "weighted avg   0.696114  0.720997  0.702224  1043.000000\n",
      "\n",
      "Guardado: eval_train_split/confusion_matrix_mobilenetv3_small.csv\n",
      "Guardado: eval_train_split/classification_report_mobilenetv3_small.csv\n",
      "\n",
      "=== EFFICIENTNET_B0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinjiro/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kevinjiro/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kevinjiro/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusión (filas=real, columnas=predicha):\n",
      "         AKIEC  BCC  BEN_OTH  BKL  DF  INF  MAL_OTH  MEL   NV  SCCKA  VASC\n",
      "AKIEC       23   19        0    4   0    0        0    3    1      9     1\n",
      "BCC          7  472        0    4   0    0        0    3    3     13     2\n",
      "BEN_OTH      1    1        2    0   0    0        0    2    1      0     1\n",
      "BKL          9   28        0   40   1    0        0   11   12      7     0\n",
      "DF           0    3        0    1   5    0        0    0    1      0     0\n",
      "INF          0    3        0    1   1    2        0    0    1      1     1\n",
      "MAL_OTH      0    0        0    0   0    0        0    1    0      0     0\n",
      "MEL          2    5        0    2   0    0        0   55   24      0     2\n",
      "NV           0   11        0    3   0    0        0   15  119      1     0\n",
      "SCCKA        4   21        0    5   0    0        0    0    0     64     0\n",
      "VASC         0    1        0    0   0    0        0    0    1      0     7\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score      support\n",
      "AKIEC          0.500000  0.383333  0.433962    60.000000\n",
      "BCC            0.836879  0.936508  0.883895   504.000000\n",
      "BEN_OTH        1.000000  0.250000  0.400000     8.000000\n",
      "BKL            0.666667  0.370370  0.476190   108.000000\n",
      "DF             0.714286  0.500000  0.588235    10.000000\n",
      "INF            1.000000  0.200000  0.333333    10.000000\n",
      "MAL_OTH        0.000000  0.000000  0.000000     1.000000\n",
      "MEL            0.611111  0.611111  0.611111    90.000000\n",
      "NV             0.730061  0.798658  0.762821   149.000000\n",
      "SCCKA          0.673684  0.680851  0.677249    94.000000\n",
      "VASC           0.500000  0.777778  0.608696     9.000000\n",
      "accuracy       0.756472  0.756472  0.756472     0.756472\n",
      "macro avg      0.657517  0.500783  0.525045  1043.000000\n",
      "weighted avg   0.748356  0.756472  0.741290  1043.000000\n",
      "\n",
      "Guardado: eval_train_split/confusion_matrix_efficientnet_b0.csv\n",
      "Guardado: eval_train_split/classification_report_efficientnet_b0.csv\n",
      "\n",
      "=== RESUMEN COMPARATIVO (ordenado por macro_f1) ===\n",
      "            model  macro_precision  macro_recall  macro_f1  weighted_precision  weighted_recall  weighted_f1  accuracy\n",
      "  efficientnet_b0         0.657517      0.500783  0.525045            0.748356         0.756472     0.741290  0.756472\n",
      "mobilenetv3_small         0.513704      0.421308  0.441312            0.696114         0.720997     0.702224  0.720997\n",
      "         resnet18         0.353786      0.300677  0.301267            0.610941         0.674976     0.635462  0.674976\n",
      "\n",
      "Guardado resumen: eval_train_split/summary_models_validation.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinjiro/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kevinjiro/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kevinjiro/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Evaluación por-lesión (late fusion) de 3 modelos usando checkpoints previos\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ---- Paths a tus checkpoints (los que ya generaste) ----\n",
    "CKPTS = {\n",
    "    \"resnet18\": \"/Users/kevinjiro/Desktop/data Piel/Model/best_resnet18.pth\",\n",
    "    \"mobilenetv3_small\": \"/Users/kevinjiro/Desktop/data Piel/Model/best_mobilenetv3_small.pth\",\n",
    "    \"efficientnet_b0\": \"/Users/kevinjiro/Desktop/data Piel/Model/best_efficientnet_b0.pth\",\n",
    "}\n",
    "\n",
    "# ---- Fábrica de arquitectura (mismo head a 11 clases) ----\n",
    "def build_model(model_name: str, num_classes: int, device=DEVICE):\n",
    "    name = model_name.lower()\n",
    "    if name == \"resnet18\":\n",
    "        model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        in_feats = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_feats, num_classes)\n",
    "    elif name == \"mobilenetv3_small\":\n",
    "        model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
    "        in_feats = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(in_feats, num_classes)\n",
    "    elif name == \"efficientnet_b0\":\n",
    "        model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        in_feats = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(in_feats, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Modelo no soportado: {model_name}\")\n",
    "    return model.to(device)\n",
    "\n",
    "def load_checkpoint_state(path: str):\n",
    "    path = Path(path)\n",
    "    assert path.exists(), f\"No existe el checkpoint: {path}\"\n",
    "    state = torch.load(path, map_location=DEVICE)\n",
    "    assert \"model\" in state, \"El checkpoint debe tener la clave 'model' con state_dict\"\n",
    "    return state[\"model\"]\n",
    "\n",
    "# ---- Predicciones por-lesión con late fusion (como en tu val) ----\n",
    "@torch.no_grad()\n",
    "def preds_per_lesion(model, loader, device=DEVICE):\n",
    "    model.eval()\n",
    "    lesion_logits_sum = defaultdict(lambda: torch.zeros(num_classes, device=device))\n",
    "    lesion_counts     = defaultdict(int)\n",
    "    lesion_targets    = {}\n",
    "\n",
    "    for xb, yb, lidb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        logits = model(xb)  # (B, C)\n",
    "        for j, lid in enumerate(lidb):\n",
    "            lesion_logits_sum[lid] += logits[j]\n",
    "            lesion_counts[lid]     += 1\n",
    "            lesion_targets[lid]     = int(yb[j].item())\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "    for lid, zsum in lesion_logits_sum.items():\n",
    "        zmean = zsum / lesion_counts[lid]\n",
    "        y_true.append(lesion_targets[lid])\n",
    "        y_pred.append(int(zmean.argmax().item()))\n",
    "\n",
    "    return np.array(y_true), np.array(y_pred)\n",
    "\n",
    "# ---- Evaluación y exportación por modelo ----\n",
    "OUT_DIR = Path(\"./eval_train_split\")  # resultados en validación\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_reports = {}   # para tenerlos en memoria también\n",
    "\n",
    "for model_name, ckpt_path in CKPTS.items():\n",
    "    print(f\"\\n=== {model_name.upper()} ===\")\n",
    "    # 1) construir arquitectura y cargar pesos\n",
    "    model = build_model(model_name, num_classes=num_classes, device=DEVICE)\n",
    "    model.load_state_dict(load_checkpoint_state(ckpt_path))\n",
    "\n",
    "    # 2) predicciones por lesión con late fusion\n",
    "    y_true, y_pred = preds_per_lesion(model, val_dl, device=DEVICE)\n",
    "\n",
    "    # 3) matriz de confusión (filas=real, columnas=predicha)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_cols))))\n",
    "    cm_df = pd.DataFrame(cm, index=class_cols, columns=class_cols)\n",
    "    print(\"\\nMatriz de confusión (filas=real, columnas=predicha):\")\n",
    "    print(cm_df)\n",
    "\n",
    "    # 4) classification report (precision, recall, f1, support) por clase\n",
    "    report_dict = classification_report(\n",
    "        y_true, y_pred, target_names=class_cols, output_dict=True, digits=3\n",
    "    )\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(report_df)\n",
    "\n",
    "    # 5) guardar a disco\n",
    "    cm_path = OUT_DIR / f\"confusion_matrix_{model_name}.csv\"\n",
    "    rep_path = OUT_DIR / f\"classification_report_{model_name}.csv\"\n",
    "    cm_df.to_csv(cm_path)\n",
    "    report_df.to_csv(rep_path)\n",
    "    print(f\"\\nGuardado: {cm_path}\")\n",
    "    print(f\"Guardado: {rep_path}\")\n",
    "\n",
    "    all_reports[model_name] = report_df\n",
    "\n",
    "# (Opcional) pequeña tabla resumen comparando macro/weighted\n",
    "summary_rows = []\n",
    "for name, rep in all_reports.items():\n",
    "    summary_rows.append({\n",
    "        \"model\": name,\n",
    "        \"macro_precision\":  rep.loc[\"macro avg\", \"precision\"],\n",
    "        \"macro_recall\":     rep.loc[\"macro avg\", \"recall\"],\n",
    "        \"macro_f1\":         rep.loc[\"macro avg\", \"f1-score\"],\n",
    "        \"weighted_precision\": rep.loc[\"weighted avg\", \"precision\"],\n",
    "        \"weighted_recall\":    rep.loc[\"weighted avg\", \"recall\"],\n",
    "        \"weighted_f1\":        rep.loc[\"weighted avg\", \"f1-score\"],\n",
    "        \"accuracy\":           rep.loc[\"accuracy\", \"precision\"] if \"accuracy\" in rep.index else None\n",
    "    })\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(\"macro_f1\", ascending=False)\n",
    "print(\"\\n=== RESUMEN COMPARATIVO (ordenado por macro_f1) ===\")\n",
    "print(summary_df.to_string(index=False))\n",
    "summary_df.to_csv(OUT_DIR / \"summary_models_validation.csv\", index=False)\n",
    "print(f\"\\nGuardado resumen: {OUT_DIR / 'summary_models_validation.csv'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
